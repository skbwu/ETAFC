{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "150bcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, copy, os, shutil, time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# glmnet implementation - for the LASSO + CV.\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot\n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "\n",
    "# for logging + dynamic assessment of results\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# are we gonna log results or nah?\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ee42186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our possible list of settings\n",
    "settings = []\n",
    "\n",
    "# iterate through all possible settings, starting with to log-transform or not\n",
    "for log_trans in [True, False]:\n",
    "    \n",
    "    # (protected local lags, protected global lags)\n",
    "    for reg_scheme in [(0, 0), (1, 0), (1, 1), (2, 0), (2, 1), (2, 2)]:\n",
    "    \n",
    "        # how many weeks are we predicting into the future?\n",
    "        for fh in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "\n",
    "            # how many historical lags are we considering when making predictions?\n",
    "            for num_lags in [1, 2, 3, 4, 6, 12, 24]:\n",
    "\n",
    "                # political (0), geographical (1), fully-connected (2), and no connections (3)\n",
    "                for cluster_mech in [0, 1, 2, 3]:\n",
    "                    \n",
    "                    # add to our list of settings\n",
    "                    settings.append([log_trans, fh, num_lags, cluster_mech, reg_scheme])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3486764b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which index are we starting at? Governed by the command line argument! 144 jobs! 346 minutes apiece.\n",
    "start_idx = int(sys.argv[1])\n",
    "\n",
    "# let's do 28x variants per job\n",
    "for i in range(start_idx*28, (start_idx*28)+28):\n",
    "    \n",
    "    # which setting are we working with?\n",
    "    log_trans, fh, num_lags, cluster_mech, reg_scheme = settings[i]\n",
    "\n",
    "    # what's the file name for this run?\n",
    "    fname = f\"fh={fh}_log-trans={log_trans}_num-lags={num_lags}_cluster-mech={cluster_mech}_reg-scheme={reg_scheme[0]}+{reg_scheme[1]}.csv\"\n",
    "\n",
    "    # let's load in our data\n",
    "    cases = pd.read_csv(\"processed/weekly_cases.csv\")\n",
    "    cases.date = pd.to_datetime(cases.date)\n",
    "    cases.set_index(\"date\", inplace=True)\n",
    "\n",
    "    # how many locations do we have?\n",
    "    num_locations = len(cases.columns)\n",
    "\n",
    "    # political state-based clustering\n",
    "    if cluster_mech == 0:\n",
    "        adj_matrix = np.loadtxt(\"processed/political_clustering/adj_matrix1.txt\")\n",
    "\n",
    "    # for distance-based clustering\n",
    "    elif cluster_mech == 1:\n",
    "        adj_matrix = np.loadtxt(\"processed/distance_clustering/adj_matrix2.txt\")\n",
    "\n",
    "    # for fully-connected clustering\n",
    "    elif cluster_mech == 2:\n",
    "        adj_matrix = np.loadtxt(\"processed/single_clustering/adj_matrix3.txt\")\n",
    "\n",
    "    # special option: just a diagonal matrix because treating each location separately!\n",
    "    elif cluster_mech == 3:\n",
    "        adj_matrix = np.eye(num_locations)\n",
    "\n",
    "    # else, throw an error\n",
    "    else:\n",
    "        raise Exception(\"The specified cluster_mech is not available.\")\n",
    "\n",
    "    # what day is it today? and what is the end of our training set?\n",
    "    today, train_end = pd.Timestamp(2020, 9, 27), pd.Timestamp(2020, 9, 27)\n",
    "\n",
    "    # what is the first point ever in our training set, always?\n",
    "    train_start = cases.iloc[:num_lags + fh].index[-1]\n",
    "\n",
    "    # specify the end of our test set\n",
    "    test_end = pd.Timestamp(2022, 5, 15)\n",
    "    \n",
    "    # create a dataframe to store our predictions\n",
    "    predictions = pd.DataFrame(data=None, columns=[\"date\"] + list(cases.columns))\n",
    "\n",
    "    # set a seed re any stochasticity / randomness\n",
    "    np.random.seed(858)\n",
    "\n",
    "    # keep track of our R2s over time\n",
    "    R2s = [np.nan]\n",
    "\n",
    "    # initialize only for verbose logging\n",
    "    if verbose:\n",
    "        weeks_remaining = np.nan\n",
    "\n",
    "    # iterate through each week in our test set until we are done.\n",
    "    while True:\n",
    "\n",
    "        # what is our prediction date based on the forecast horizon?\n",
    "        pred_date = today + relativedelta(days=7*fh)\n",
    "\n",
    "        # check if we've already made all the predictions we need\n",
    "        if pred_date > test_end:\n",
    "            break\n",
    "\n",
    "        # let's do a quick sanity check to see if this is actually a feasible time to start training + making predictions ...\n",
    "        if train_start <= today:\n",
    "\n",
    "            # create a list to store our predictions for each location AT THIS TIMESTEP!\n",
    "            preds = []\n",
    "\n",
    "            # the move is to train + predict for each location individually\n",
    "            for loc_idx in range(num_locations):\n",
    "\n",
    "                ######## TRAINING OUR MODEL (FOR THIS LOCATION ONLY) ###########\n",
    "\n",
    "                # create a data structure to encompass the X_train later\n",
    "                X_train = []\n",
    "\n",
    "                # build x_train and penalty_factor vectors for each training date\n",
    "                for train_date in cases.loc[train_start : today].index:\n",
    "\n",
    "                    # figure out which lags are we not regularizing for each of local vs. neighbor lags\n",
    "                    local_reg, neighbor_reg = reg_scheme\n",
    "\n",
    "                    # start our x_train point & our regularization vector\n",
    "                    x_train = np.array([])\n",
    "                    penalty_factor = np.array([])\n",
    "\n",
    "                    # get all possible cases during the num_lags corresponding to this training point\n",
    "                    avail_lags = cases.loc[train_date - relativedelta(days=7*(num_lags + fh - 1)) : \\\n",
    "                                           train_date - relativedelta(days=7*fh)]\n",
    "\n",
    "                    # add in the self-loop first corresponding to local lags, also the penalty_factor\n",
    "                    x_train = np.concatenate([x_train, copy.deepcopy(avail_lags[cases.columns[loc_idx]].values)])\n",
    "\n",
    "                    # add in the local lags regularizer\n",
    "                    penalty_factor_loc = np.ones(num_lags)\n",
    "                    if (local_reg > 0) and (local_reg < num_lags):\n",
    "                        penalty_factor_loc[-np.minimum(local_reg, num_lags):] = 0.0\n",
    "                    penalty_factor = np.concatenate([penalty_factor, copy.deepcopy(penalty_factor_loc)])\n",
    "\n",
    "                    # which locations are connected to this loc_idx?\n",
    "                    relevant_neighbor_idxs = np.argwhere(adj_matrix[loc_idx] == 1.0).flatten()\n",
    "                    relevant_neighbor_idxs = relevant_neighbor_idxs[relevant_neighbor_idxs != loc_idx]\n",
    "\n",
    "                    # add in neighbor lags + the corresponding penalty_factor_neighbor\n",
    "                    for neighbor_idx in relevant_neighbor_idxs:\n",
    "\n",
    "                        # add this neighboring location's lags in\n",
    "                        x_train = np.concatenate([x_train, copy.deepcopy(avail_lags[cases.columns[neighbor_idx]].values)])\n",
    "\n",
    "                        # create the neighbor logs regularizer\n",
    "                        penalty_factor_neighbor = np.ones(num_lags)\n",
    "                        if (neighbor_reg > 0) and (neighbor_reg < num_lags):\n",
    "                            penalty_factor_neighbor[-np.minimum(neighbor_reg, num_lags):] = 0.0\n",
    "                        penalty_factor = np.concatenate([penalty_factor, copy.deepcopy(penalty_factor_neighbor)])\n",
    "\n",
    "                    # add to our X_train structure\n",
    "                    X_train.append(x_train)\n",
    "\n",
    "                # assemble our X_train completely + compute our y_train FOR THIS LOCATION ONLY\n",
    "                X_train = np.array(X_train)\n",
    "                y_train = cases.loc[train_start : today][[cases.columns[loc_idx]]].values\n",
    "\n",
    "                # check if we need to log-transform!\n",
    "                if log_trans == True:\n",
    "                    X_train = np.log1p(X_train)\n",
    "                    y_train = np.log1p(y_train)\n",
    "\n",
    "                # train our model using cross-validated lasso\n",
    "                cvfit = cvglmnet(x = X_train, y = y_train, alpha = 1, nlambda = 10000, \n",
    "                                 penalty_factor = penalty_factor,\n",
    "                                 ptype = 'mse', nfolds = 5)\n",
    "\n",
    "                ######## PREDICTING WITH OUR MODEL (FOR THIS LOCATION ONLY) ###########\n",
    "\n",
    "                # assemble x_test features for making our prediction AS A ROW VECTOR!\n",
    "                x_test = np.array([])\n",
    "\n",
    "                # get all possible cases during the num_lags corresponding to this PREDICTION POINT\n",
    "                avail_lags = cases.loc[pred_date - relativedelta(days=7*(num_lags + fh - 1)) : \\\n",
    "                                       pred_date - relativedelta(days=7*fh)]\n",
    "\n",
    "                # add in the self-loop first corresponding to local lags, also the penalty_factor\n",
    "                x_test = np.concatenate([x_test, copy.deepcopy(avail_lags[cases.columns[loc_idx]].values)])\n",
    "\n",
    "                # add in neighbor lags\n",
    "                for neighbor_idx in relevant_neighbor_idxs:\n",
    "\n",
    "                    # add this neighboring location's lags in\n",
    "                    x_test = np.concatenate([x_test, copy.deepcopy(avail_lags[cases.columns[neighbor_idx]].values)])\n",
    "\n",
    "                # cast to a ROW VECTOR + potentially log-transforming\n",
    "                x_test = x_test.reshape(1, -1)\n",
    "                if log_trans == True:\n",
    "                    x_test = np.log1p(x_test)\n",
    "\n",
    "                # make our prediction\n",
    "                pred = cvglmnetPredict(cvfit, newx = x_test, s='lambda_1se')\n",
    "\n",
    "                # check if we need to reverse the log-transform\n",
    "                if log_trans == True:\n",
    "                    pred = np.expm1(pred)\n",
    "\n",
    "                # add to our list\n",
    "                preds.append(pred[0, 0])\n",
    "\n",
    "            # add to our dataframe\n",
    "            predictions.loc[len(predictions.index)] = [pred_date] + preds\n",
    "\n",
    "            # compute R^2 to see how good we're doing ...\n",
    "            SS_tot = ((cases.loc[pred_date] - cases.loc[pred_date].values.mean()) ** 2).sum()\n",
    "            SS_res = ((np.array(preds) - cases.loc[pred_date].values) ** 2).sum()\n",
    "            R2s.append( 1 - (SS_res / SS_tot) )\n",
    "\n",
    "            # logging\n",
    "            if verbose == True:\n",
    "\n",
    "                # how many weeks remaining?\n",
    "                weeks_remaining = ((test_end - pred_date) / 7).days\n",
    "\n",
    "                # check our status\n",
    "                clear_output(wait=True)\n",
    "                print(f\"{pred_date} | {weeks_remaining} weeks remaining. Most recent R^2: {R2s[-1]}\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            # just create a flag just says we didn't have enough days\n",
    "            print(\"Not enough observations to start training yet!\")\n",
    "\n",
    "        # increment what today is\n",
    "        today += relativedelta(days=7)\n",
    "\n",
    "    # save our logs at the very end\n",
    "    predictions.to_csv(f\"linear_results/{fname}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (M249)",
   "language": "python",
   "name": "m249"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
